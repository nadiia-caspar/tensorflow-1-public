# Convolutional Neural Networks in TensorFlow (Course 2 of the TensorFlow Developer Specialization)

## Week 1

### My course notes

### Learn more...

## Week 2

### My course notes

* **Augmentation:** a technique to avoid overfitting

* **Image augmentation** is a technique commonly used in the field of computer vision and image processing to increase the diversity and size of a dataset. It involves applying various transformations and modifications to the original images to create new, slightly altered versions of the same data. These augmented images can then be used for training machine learning models, particularly for tasks like object detection, image classification, and segmentation.

### Learn more...

To learn more about Augmentation, and the available transforms, check out <https://keras.io/api/layers/preprocessing_layers/>
-- and note that it's referred to as preprocessing for a very powerful reason: that it doesn't require you to edit your
raw images, nor does it amend them for you on-disk. It does it in-memory as it's performing the training, allowing you
to experiment without impacting your dataset.


## Week 3

### My course notes

* **Transfer learning** - Transfer learning in neural networks refers to a technique where a pre-trained model that has
been trained on a large dataset is adapted to perform a new task or on a new dataset related to the original task.

* **Dropout** - Dropout is a regularization technique used in neural networks to prevent overfitting. Overfitting occurs
when a neural network learns to perform exceptionally well on the training data but fails to generalize to new, unseen
data. Dropout helps mitigate this problem by randomly deactivating (dropping out) a proportion of neurons during each
training iteration.

### Learn more...

Using dropout: Check out Andrew's terrific video explaining dropouts here: <https://www.youtube.com/watch?v=ARq74QuavAo>

## Week 4

### My course notes

### Learn more...